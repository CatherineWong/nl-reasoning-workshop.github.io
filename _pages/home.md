---
layout: project
urltitle:  "Visually Grounded Interaction and Language (ViGIL)"
title: "Visually Grounded Interaction and Language (ViGIL)"
categories: workshop, computer vision, natural language, grounding, interaction, machine learning, vigil, naacl, 2021
permalink: /
bibtex: true
paper: true
acknowledgements: ""
---

<br />
<div class="row">
  <div class="col-xs-12">
    <center><h1>Visually Grounded Interaction and Language (ViGIL)</h1></center>
    <center><h2>June 10, 2021. NAACL Workshop, Mexico City, Mexico.</h2></center>
  </div>
</div>

<br />

<div class="row">
    <div class="col-xs-12">
        <p>
          VIGIL is a one-day interdisciplinary workshop that will push the boundaries of language grounding systems. We aim to bridge the fields of human cognition and machine learning through discussions on combining language, perception and other modalities via interaction. This year, ViGIL will also be hosting the <a href="https://cs.stanford.edu/people/dorarad/gqa/challenge.html">2nd GQA challenge</a>, which focuses on compositional reasoning for visual question answering.
        </p>
    </div>
</div>

<br />

<div class="row" id="schedule">
  <div class="col-md-4 col-xs-12">
    <h2>Schedule</h2>
  </div>
  <div class="col-md-8 col-xs-12">
      <select id="timezone-select" class="form-control"></select>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped" id="schedule-table">
    <tbody>
    <tr> <th scope="row" data-time="08:50">08:50 AM</th> <td>Opening Remarks</td></tr>
    <tr> <th scope="row" data-time="09:00">09:00 AM</th> <td>Talk 1: Roger Levy</td></tr>
    <tr> <th scope="row" data-time="09:45">09:45 AM</th> <td>Talk 2: Stefanie Tellex</td></tr>
    <tr> <th scope="row" data-time="10:30">10:30 AM</th> <td>Break 1</td></tr>
    <tr> <th scope="row" data-time="11:00">11:00 AM</th> <td>Talk 3: Katerina Fragkiadaki</td> </tr>
    <tr> <th scope="row" data-time="11:45">11:45 AM</th> <td>Talk 4: Max Garagnani</td></tr>
    <tr> <th scope="row" data-time="12:30">12:30 PM</th> <td>Break 2</td></tr>
    <tr> <th scope="row" data-time="13:00">13:00 PM</th> <td>Panel Discussion</td> </tr>
    <tr> <th scope="row" data-time="14:00">14:00 PM</th> <td>Break 3</td> </tr>
    <tr> <th scope="row" data-time="14:30">14:30 PM</th> <td>Talk 5: Yejin Choi</td> </tr>
    <tr> <th scope="row" data-time="15:15">15:15 PM</th> <td>Talk 6: Justin Johnson</td> </tr>
    <tr> <th scope="row" data-time="16:00">16:00 PM</th> <td>GQA Challenge</td> </tr>
    <tr> <th scope="row" data-time="16:20">16:20 PM</th> <td>Spotlight Presentations</td> </tr>
    <tr> <th scope="row" data-time="16:30">16:30 PM</th> <td>Poster</td> </tr>
    <tr> <th scope="row" data-time="18:00">18:00 PM</th> <td>Talk 7: Trevor Darrell</td> </tr>
    <tr> <th scope="row" data-time="18:45">18:45 PM</th> <td>Talk 8: Sandra Waxman</td> </tr>
    <tr> <th scope="row" data-time="19:30">19:30 PM</th> <td>Closing Remark</td> </tr>
    </tbody>
    </table>
  </div>
</div>

<hr />

<!--div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td><del>March 19, 2021</del></td>
        </tr>
        <tr>
          <td>Decision Notifications</td>
          <td><del>April 15, 2021</del></td>
        </tr>
        <tr>
          <td>Camera Ready Paper Deadline</td>
          <td><b>May 15, 2021 (11:59 PM Pacific time)</b></td>
        </tr>
        <tr>
          <td>Workshop</td>
          <td>June 10, 2021</td>
        </tr>
      </tbody>
    </table>
  </div>
</div-->

<!-- Speakers -->
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Speakers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-6 col-lg-3">
    <a href="https://homes.cs.washington.edu/~yejin/">
      <img class="people-pic" src="{{ "/static/img/people/yejin-choi.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
      <h6>University of Washington</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://people.eecs.berkeley.edu/~trevor/">
      <img class="people-pic" src="{{ "/static/img/people/darrell.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
      <h6>Berkeley</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://www.cs.cmu.edu/~katef/">
      <img class="people-pic" src="{{ "/static/img/people/katerina-fragkiadaki.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>
      <h6>CMU</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://www.gold.ac.uk/computing/people/garagnani-max/">
      <img class="people-pic" src="{{ "/static/img/people/max-garagnani.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.gold.ac.uk/computing/people/garagnani-max/">Max Garagnani</a>
      <h6>Goldsmiths, University of London</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://web.eecs.umich.edu/~justincj/">
      <img class="people-pic" src="{{ "/static/img/people/justin-johnson.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>
      <h6>University of Michigan</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www.mit.edu/~rplevy/">
      <img class="people-pic" src="{{ "/static/img/people/roger-levy.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.mit.edu/~rplevy/">Roger Levy</a>
      <h6>MIT</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://cs.brown.edu/people/stellex/">
      <img class="people-pic" src="{{ "/static/img/people/stefanie-tellex.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://cs.brown.edu/people/stellex/">Stephanie Tellex</a>
      <h6>Brown University</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://childdevelopment.northwestern.edu/">
      <img class="people-pic" src="{{ "/static/img/people/sandra-waxman.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://childdevelopment.northwestern.edu/">Sandra Waxman</a>
      <h6>Northwestern University</h6>
    </div>
  </div>
</div>

<hr />

<div class="row" id="accepted">
  <div class="col-xs-12">
    <h2>Accepted Papers</h2>
  </div>
</div>
<ul class="paper-list">
    <li>
        <span class="paper-title"><span style="color:#DD3333;font-weight:700">[Spotlight]</span> Emergent Communication of Generalizations</span><br>
        <span class="paper-authors">Jesse Mu (Stanford University); Noah   Goodman (Stanford University)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/7.pdf">PDF</a>] [<a href="static/papers-2021/7_supp.pdf">Supp</a>] [<a href="https://www.youtube.com/watch?v=I1UXGmz3rUk" target="_blank">Spotlight Video</a>]</span>
    </li>
    <li>
        <span class="paper-title"><span style="color:#DD3333;font-weight:700">[Spotlight]</span> SocialAI 0.1: Towards a Benchmark to Stimulate Research on Socio-Cognitive Abilities in Deep Reinforcement Learning Agents</span><br>
        <span class="paper-authors">Rémy Portelas (Inria Bordeaux); Grgur Kovač (INRIA, Flowers team); Katja Hofmann (Microsoft Research); Pierre-Yves Oudeyer (Inria)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/20.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title"><span style="color:#DD3333;font-weight:700">[Spotlight]</span> Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task Feasibility in Interactive Visual Environments</span><br>
        <span class="paper-authors">Andrea Burns (Boston University); Deniz Arsan (University of Illinois at Urbana Champaign); Sanjna Agrawal (Boston University); Ranjitha Kumar (UIUC: CS); Kate Saenko (Boston University); Bryan Plummer (Boston University)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/26.pdf">PDF</a>] [<a href="static/papers-2021/26_video.mp4" target="_blank">Spotlight Video</a>]</span>
    </li>
    <li>
        <span class="paper-title">Curriculum Learning for Vision-Grounded Instruction Following</span><br>
        <span class="paper-authors">Guan-Lin Chao (Carnegie Mellon University); Ian Lane (Carnegie Mellon University)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/3.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">RefineCap: Concept-Aware Refinement for Image Captioning</span><br>
        <span class="paper-authors">Yekun Chai (Institute of Automation, Chinese Academy of Sciences); Shuo Jin (University of Pittsburgh); Junliang Xing (Institute of Automation, Chinese Academy of Sciences)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/4.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Image Translation Model</span><br>
        <span class="paper-authors">Puneet Jain (Google); Orhan Firat (Google ); Qi Ge (Google); Sihang Liang (Princeton University)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/5.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">“Yes” and “No”: Visually Grounded Polar Answers</span><br>
        <span class="paper-authors">Claudio  Greco (University of Trento); Alberto Testoni (University of Trento); Raffaella Bernardi (University of Trento)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/8.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator</span><br>
        <span class="paper-authors">Ayush Shrivastava (Georgia Institute of Technology); Karthik Gopalakrishnan (Amazon Alexa AI); Yang Liu (Amazon, Alexa AI); Robinson Piramuthu (eBay Inc.); Gokhan Tur (); Devi Parikh (Georgia Tech & Facebook AI Research); Dilek Hakkani-Tur (Amazon Alexa AI)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/9.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Interactive Learning from Activity Description</span><br>
        <span class="paper-authors">Khanh X Nguyen (University of Maryland); Dipendra Misra (Microsoft); Robert Schapire (Microsoft); Miroslav Dudik (Microsoft); Patrick Shafto (Rutgers University-Newark)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/10.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Language-Conditional Imitation Learning</span><br>
        <span class="paper-authors">Julian Skirzynski (University of California, San Diego)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/11.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Learning a natural-language to LTL executable semantic parser for grounded robotics</span><br>
        <span class="paper-authors">Christopher Wang (MIT); Candace Ross (Massachusetts Institute of Technology); Yen-Ling Kuo (MIT); Boris Katz (MIT); Andrei Barbu (MIT)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/12.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Zero-Shot Generalization using Intrinsically Motivated Compositional Emergent Protocols</span><br>
        <span class="paper-authors">Rishi Hazra (IISc Bangalore); Sonu Dixit (Indian Institue of Science); Sayambhu Sen (IISc, Bangalore)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/13.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Pose-Guided Sign Language Video GAN with Dynamic Lambda</span><br>
        <span class="paper-authors">Christopher Kissel (Beuth University of Applied Sciences Berlin); Christopher Kümmel (Beuth University of Applied Sciences Berlin); Dennis Ritter (Beuth University of Applied Sciences Berlin); Kristian Hildebrand (Beuth University of Applied Sciences Berlin)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/14.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Extracting Phone Numbers from Adversarial & Visually Corrupted Text</span><br>
        <span class="paper-authors">Timothy Forman (United States Naval Academy); Nathanael Chambers (United States Naval Academy)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/15.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">gComm: An environment for investigating generalization in grounded language acquisition</span><br>
        <span class="paper-authors">Rishi Hazra (IISc Bangalore); Sonu Dixit (Indian Institue of Science)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/16.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Locate then Segment: A Strong Pipeline for Referring Image Segmentation</span><br>
        <span class="paper-authors">Ya Jing (Institute of Automation, Chinese Academy of Sciences); Tao Kong (Bytedance); Wei Wang (Institute of Automation Chinese Academy of Sciences); Liang Wang (NLPR, China); Lei Li (ByteDance AI Lab); Tieniu Tan (NLPR, China)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/18.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Towards Multi-Modal Text-Image Retrieval to improve Human Reading</span><br>
        <span class="paper-authors">Florian Schneider (University of Hamburg); Ozge Alacam (University of Hamburg); Xintong Wang (University of Hamburg); Chris Biemann (University of Hamburg)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/19.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Attend, tell and ground: Weakly-supervised Object Grounding with Attention-based Conditional Variational Autoencoders</span><br>
        <span class="paper-authors">Effrosyni Mavroudi (Johns Hopkins University); Rene Vidal (Johns Hopkins University, USA)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/21.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Referring to the recently seen: reference and perceptual memory in situated dialog</span><br>
        <span class="paper-authors">John Kelleher (Technological University Dublin); Simon Dobnik (University of Gothenburg); John D Kelleher (Technological University Dublin)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/22.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">How Important are Visual Features for Visual Grounding? It Depends.</span><br>
        <span class="paper-authors">Fan Yang (Amazon); Prashan Wanigasekara (Amazon); Mingda Li (Amazon); Chengwei  Su (Amazon); Emre Barut (Amazon)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/24.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">Leveraging Language for Abstraction and Program Search</span><br>
        <span class="paper-authors">Catherine Wong (Massachusetts Institute of Technology); Kevin M Ellis (MIT); Jacob Andreas (MIT); Joshua Tenenbaum (MIT)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/25.pdf">PDF</a>]</span>
    </li>
    <li>
        <span class="paper-title">SynthRef: Generation of Synthetic Referring Expressions for Object Segmentation</span><br>
        <span class="paper-authors">Ioannis Kazakos (National Technical University of Athens); Carles Ventura (Universitat Oberta de Catalunya); Miriam Bellver (Barcelona Supercomputing Center); Carina Silberer (Institute for Natural Language Processing, University of Stuttgart); Xavier Giro-i-Nieto (Universitat Politecnica de Catalunya)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/27.pdf">PDF</a>] [<a href="static/papers-2021/27_supp.pdf">Supp</a>]</span>
    </li>
    <li>
        <span class="paper-title">Do Videos Guide Translations?  Evaluation of a Video-Guided Machine Translation dataset</span><br>
        <span class="paper-authors">Zhishen Yang (Tokyo Institute of Technology); Tosho  Hirasawa (Tokyo Metropolitan University); Naoaki Okazaki (Tokyo Institute of Technology); Mamoru Komachi (Tokyo Metropolitan University)</span><br>
        <span class="paper-meta">[<a href="static/papers-2021/29.pdf">PDF</a>]</span>
    </li>
</ul>

<hr />

<!-- Organizers -->
<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-6 col-lg-3">
    <a href="https://catalinacangea.netlify.app/">
      <img class="people-pic" src="{{ "/static/img/people/catalinacangea.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://catalinacangea.netlify.app/">Cătălina Cangea</a>
      <h6>University of Cambridge</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://abhishekdas.com">
      <img class="people-pic" src="{{ "/static/img/people/abhishek-das-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://abhishekdas.com">Abhishek Das</a>
      <h6>Facebook AI Research</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://www.linkedin.com/in/drew-a-hudson">
      <img class="people-pic" src="{{ "/static/img/people/drew.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/drew-a-hudson">Drew Hudson</a>
      <h6>Stanford</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://jacobkrantz.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/jacobkrantz.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://jacobkrantz.github.io/">Jacob Krantz</a>
      <h6>Oregon State University</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://www.cc.gatech.edu/~slee3191/">
      <img class="people-pic" src="{{ "/static/img/people/stefan.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>
      <h6>Oregon State University</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://jiayuanm.com/">
      <img class="people-pic" src="{{ "/static/img/people/jiayuanmao.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://jiayuanm.com/">Jiayuan Mao</a>
      <h6>MIT</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="https://fstrub95.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/florianstrub.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fstrub95.github.io/">Florian Strub</a>
      <h6>DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://alanesuhr.com/">
      <img class="people-pic" src="{{ "/static/img/people/alane.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://alanesuhr.com/">Alane Suhr</a>
      <h6>Cornell</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://wijmans.xyz/">
      <img class="people-pic" src="{{ "/static/img/people/erikwijmans.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://wijmans.xyz/">Erik Wijmans</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>
</div>

<hr />

<!-- Scientific Committee -->
<div class="row" id="scientific_committee">
  <div class="col-xs-12">
    <h2>Scientific Committee</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-6 col-lg-3">
    <a href="https://mila.quebec/en/person/aaron-courville/">
      <img class="people-pic" src="{{ "/static/img/people/aaron-courville-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://mila.quebec/en/person/aaron-courville/">Aaron Courville</a>
      <h6>University of Montreal</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www.mateuszmalinowski.com/">
      <img class="people-pic" src="{{ "/static/img/people/mateusz-malinowski-dp.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.mateuszmalinowski.com/">Mateusz Malinowski</a>
      <h6>DeepMind</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www.lifl.fr/~pietquin/">
      <img class="people-pic" src="{{ "/static/img/people/olivier-pietquin-dp.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.lifl.fr/~pietquin/">Olivier Pietquin</a>
      <h6>Google Brain</h6>
    </div>
  </div>
  <div class="col-xs-6 col-lg-3">
    <a href="http://www-etud.iro.umontreal.ca/~devries/">
      <img class="people-pic" src="/static/img/people/harmdevries.jpg" />
    </a>
    <div class="people-name">
      <a href="http://www-etud.iro.umontreal.ca/~devries/">Harm de Vries</a>
      <h6>University of Montreal | Element AI</h6>
    </div>
  </div>
</div>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Program Committee</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-6">
    <ul>
      <li>Abhishek Das (Facebook AI Research)</li>
      <li>Adria Recasens (DeepMind)</li>
      <li>Alane Suhr (Cornell)</li>
      <li>Anna Potapenko (DeepMind)</li>
      <li>Arjun Majumdar (Georgia Tech)</li>
      <li>Cătălina Cangea (University of Cambridge)</li>
      <li>Catherine Wong (Massachusetts Institute of Technology)</li>
      <li>Christopher Davis (University of Cambridge)</li>
      <li>Daniel Fried (UC Berkeley)</li>
      <li>Drew Hudson (Stanford University)</li>
      <li>Erik Wijmans (Georgia Tech)</li>
      <li>Florian Strub (DeepMind)</li>
      <li>Gabriel Ilharco (University of Washington)</li>
      <li>Geoffrey Cideron (InstaDeep)</li>
      <li>Hammad Ayyubi (Columbia University)</li>
      <li>Hao Tan (University of North Carolina Chapel Hill)</li>
      <li>Hao Wu (Fudan University)</li>
      <li>Haoyue Shi (Toyota Technological Institute at Chicago)</li>
      <li>Hedi Ben-younes (Sorbonne université)</li>
      <li>Jack Hessel (Allen Institute for AI)</li>
      <li>Jacob Krantz (Oregon State University)</li>
      <li>Jean-Baptiste Alayrac (DeepMind)</li>
    </ul>
  </div>
  <div class="col-xs-6">
    <ul>
      <li>Jiayuan Mao (MIT)</li>
      <li>Joel Ye (Georgia Tech)</li>
      <li>Johan Ferret (Google Research, Brain Team)</li>
      <li>Karan Desai (University of Michigan)</li>
      <li>Lisa Anne Hendricks (DeepMind)</li>
      <li>Luca Celotti (Université de Sherbrooke)</li>
      <li>Mateusz Malinowski (DeepMind)</li>
      <li>Mathieu Rita (École polytechnique)</li>
      <li>Mathieu Seurin (University of Lille)</li>
      <li>Meera Hahn (Georgia Institute of Technology)</li>
      <li>Nicholas Tomlin (UC Berkeley)</li>
      <li>Olivier Pietquin (2)</li>
      <li>Peter Anderson (Google)</li>
      <li>Rodolfo Corona (UC Berkeley)</li>
      <li>Rowan Zellers (University of Washington)</li>
      <li>Ryan Benmalek (Cornell University)</li>
      <li>Sanjay Subramanian (Allen Institute for Artificial Intelligence)</li>
      <li>Sidd Karamcheti (Stanford University)</li>
      <li>Stefan Lee (Oregon State University)</li>
      <li>Valts Blukis (Cornell University)</li>
      <li>Volkan Cirik (Carnegie Mellon University)</li>
    </ul>
  </div>
</div>

<hr />

<!-- CfP -->
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The authors are welcome to submit a 4-page paper based on in-progress work, or relevant paper being presented at the main conference, on any of the following topics:
    </p>
    <p>
          <ul>
  <li>grounded and interactive language acquisition;</li>
  <li>reasoning and planning in language, vision, and interactive domains;</li>
  <li>machine translation with visual cues;</li>
  <li>transfer learning in language and vision tasks;</li>
  <li>visual captioning, dialog, storytelling, and question-answering;</li>
  <li>visual synthesis from language;</li>
  <li>embodied agents: language instructions, agent co-ordination through language, interaction;</li>
  <li>language-grounded robotic learning with multimodal inputs;</li>
  <li>human-machine interaction with language through robots or within virtual world;</li>
  <li>audio-visual scene understanding and dialog systems;</li>
  <li>novel tasks that combine language, vision, interactions, and other modalities;</li>
  <li>understanding and modeling the relationship between language and vision in humans;</li>
  <li>semantic systems and modeling of natural language and visual stimuli representations in the human brain;</li>
  <li>epistemology and research reflexions about language grounding, human embodiment and other related topics</li>
  <li>visual and linguistic cognition in infancy and/or adults</li>
          </ul>
      </p>
      <p>We welcome review and positional papers that may foster discussions. We also encourage published papers from <i>*non-ML*</i> conferences, e.g. epistemology, cognitive science, psychology, neuroscience, that are within the scope of the workshop. Accepted papers will be presented during joint poster sessions, with exceptional submissions selected for spotlight oral presentations. Accepted papers will be made publicly available as <i>*non-archival*</i> reports, allowing future submissions to archival conferences or journals. </p>
  </div>
</div>

<hr />

<!-- Submission -->
<div class="row" id="guidelines">
  <div class="col-xs-12">
    <h2>Submission Guidelines</h2>
  </div>
</div>
<div class="row">
    <div class="col-xs-12">
      <p>
            Please upload submissions at: <a style="color:#2980b9;font-weight:400;" href="https://cmt3.research.microsoft.com/VIGIL2021/">cmt3.research.microsoft.com/VIGIL2021</a>.
        </p>
    <ul>
      <li><b>Previously published work</b>: We welcome previously published papers from non-ML conferences, will also accept cross-submissions from ML conferences (including NAACL 2021) which are within the scope of the workshop without re-formatting. These specific papers do not have to be anonymous. They are eligible for poster sessions and will only have a very light review process.</li>
      <li><b>Unpublished work</b>: All submissions must be in PDF format. The submissions must be formated using the <a style="color:#2980b9;font-weight:400;" href="https://2021.naacl.org/calls/style-and-formatting/">NAACL 2021 LaTeX style file</a>. Submissions are limited to 4 content pages, including all figures and tables; additional pages containing statements of acknowledgements and funding disclosures, and references are allowed. The maximum file size for submissions is 50MB. The CMT-based review process will be double-blind to avoid potential conflicts of interests.</li>
    </ul>
    <p>
        In case of any issues, feel free to email the workshop organizers at: <a href="mailto:vigilworkshop@gmail.com">vigilworkshop@gmail.com</a>.
    </p>
    </div>
</div>

<hr />
<!-- Intro -->
<div class="row" id="intro">
    <div class="col-xs-12">
        <h2>Introduction</h2>
        <p>Language is neither learned nor used in a vacuum, but rather grounded within a rich, embodied experience rife with physical groundings (vision, audition, touch) and social influences (pragmatic reasoning about interlocutors, commonsense reasoning, learning from interaction) [1]. For example, studies of language acquisition in children show a strong interdependence between perception, motor control, and language understanding [2]. Yet, AI research has traditionally carved out individual components of this multimodal puzzle—perception (computer vision, audio processing, haptics), interaction with the world or other agents (robotics, reinforcement learning), and natural language processing—rather than adopting an interdisciplinary approach.</p>
        <p>This fractured lens makes it difficult to address key language understanding problems that future agents will face in the wild. For example, describing "a bird perched on the lowest branch singing in a high pitch trill" requires grounding to perception. Likewise, providing the instruction to "move the jack to the left so it pushes on the frame of the car" requires not only perceptual grounding, but also physical understanding. For these reasons, language, perception, and interaction should be learned and bootstrapped together. In the last several years, efforts to merge subsets of these areas have gained popularity through tasks like instruction-guided navigation in 3D environments [3–5], audio-visual navigation [6], video descriptions [7], question-answering [8–11], and language-conditioned robotic control [12, 13], though these primarily study disembodied problems via static datasets. As such, there remains considerable scientific uncertainty around how to bridge the gap from current monolithic systems to holistic agents. What are the tasks? The environments? How to design and train such models? To transfer knowledge between modalities? To perform multimodal reasoning? To deploy language agents in the wild?</p>
        <p>As in past incarnations, the goal of this 4th ViGIL workshop is to support and promote this research direction by bringing together scientists from diverse backgrounds—natural language processing, machine learning, computer vision, robotics, neuroscience, cognitive science, psychology, and philosophy—to share their perspectives on language grounding, embodiment, and interaction. ViGIL provides a unique opportunity for interdisciplinary discussion. We intend to utilize this variety of perspectives to foster new ideas about how to define, evaluate, learn, and leverage language grounding. This one-day session would enable in-depth conversations on understanding the boundaries of current work and establishing promising avenues for future work, with the overall aim to bridge the scientific fields of human cognition and machine learning.</p>
    </div>
</div>

<hr />

<!-- Previous Sessions -->
<div class="row">
  <div class="col-xs-12">
    <h2>Previous Sessions</h2><a name="/prev_session"></a>
  </div>
</div>
<div class="row">
    <div class="col-xs-12">
        <p>
            <ul>
                <li><a href="https://nips2017vigil.github.io/">ViGIL Workshop at NeurIPS 2017</a></li>
                <li><a href="https://nips2018vigil.github.io/">ViGIL Workshop at NeurIPS 2018</a></li>
                <li><a href="https://vigilworkshop.github.io/2019">ViGIL Workshop at NeurIPS 2019</a></li>
            </ul>
        </p>
    </div>
</div>

<hr />

<!-- References -->
<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <ol>
<li>Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto,
and J. Turian. Experience grounds language. In EMNLP, 2020. </li>
<li>L. Smith and M. Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13–29,
2005. </li>
<li>P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-andlanguage Navigation: Interpreting Visually-grounded Navigation Instructions in Real Environments. In CVPR, 2018. </li>
<li>A. Suhr, C. Yan, J. Schluger, S. Yu, H. Khader, M. Mouallem, I. Zhang, and Y. Artzi. Executing instructions in situated
collaborative interactions. In EMNLP-IJCNLP, 2019. </li>
<li>D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkhin, and Y. Artzi. Mapping instructions to actions in 3D environments with visual goal prediction. In EMNLP, 2018. </li>
<li>C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu, P. Robinson, and K. Grauman. SoundSpaces:
Audio-Visual Navigation in 3D Environments. ECCV, 2019. </li>
<li>S. Venugopalan, M. Rohrbach, J. Donahue, R. J. Mooney, T. Darrell, and K. Saenko. Sequence to sequence-video to text.
In ICCV, 2015. </li>
<li>S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, L. C. Zitnick, and D. Parikh. VQA: Visual question answering. In CVPR,
2015.</li>
<li>A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Embodied Question Answering. In CVPR, 2018. </li>
<li>R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi. From recognition to cognition: Visual commonsense reasoning. In CVPR,
2019. </li>
<li>J. Lei, L. Yu, M. Bansal, and T. L. Berg. TVQA: Localized, compositional video question answering. In EMNLP, 2018. </li>
<li>S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek. Robots that use language. Annual Review of Control, Robotics, and
Autonomous Systems, 3:25–55, 2020. </li>
<li>V. Blukis, Y. Terme, E. Niklasson, R. A. Knepper, and Y. Artzi. Learning to map natural language instructions to physical
quadcopter control using simulated flight. In CoRL, 2019. </li>
    </ol>
  </div>
</div>
